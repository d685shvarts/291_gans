{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd\n",
    "import time\n",
    "import os\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib_is_available = True\n",
    "try:\n",
    "  from matplotlib import pyplot as plt\n",
    "except ImportError:\n",
    "  print(\"Will skip plotting; matplotlib is not available.\")\n",
    "  matplotlib_is_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data params\n",
    "data_mean = 10\n",
    "data_stddev = 5\n",
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data [Only 4 moments]\n"
     ]
    }
   ],
   "source": [
    "# ### Uncomment only one of these to define what data is actually sent to the Discriminator\n",
    "#(name, preprocess, d_input_func) = (\"Raw data\", lambda data: data, lambda x: x)\n",
    "#(name, preprocess, d_input_func) = (\"Data and variances\", lambda data: decorate_with_diffs(data, 2.0), lambda x: x * 2)\n",
    "#(name, preprocess, d_input_func) = (\"Data and diffs\", lambda data: decorate_with_diffs(data, 1.0), lambda x: x * 2)\n",
    "(name, preprocess, d_input_func) = (\"Only 4 moments\", lambda data: get_moments(data), lambda x: 4)\n",
    "print(\"Using data [%s]\" % (name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution_sampler(mu, sigma):\n",
    "    return lambda n: torch.Tensor(np.random.normal(mu, sigma, (1, n)))  # Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator_input_sampler():\n",
    "    return lambda m, n: torch.rand(m, n)  # Uniform-dist data into generator, _NOT_ Gaussian\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.map1(x))\n",
    "        x = F.sigmoid(self.map2(x))\n",
    "        return self.map3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.map1(x))\n",
    "        x = F.elu(self.map2(x))\n",
    "        # return F.sigmoid(self.map3(x))\n",
    "        return self.map3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(v):\n",
    "    return v.data.storage().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(d):\n",
    "    return [np.mean(d), np.std(d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moments(d):\n",
    "    # Return the first 4 moments of the data provided\n",
    "    mean = torch.mean(d)\n",
    "    diffs = d - mean\n",
    "    var = torch.mean(torch.pow(diffs, 2.0))\n",
    "    std = torch.pow(var, 0.5)\n",
    "    zscores = diffs / std\n",
    "    skews = torch.mean(torch.pow(zscores, 3.0))\n",
    "    kurtoses = torch.mean(torch.pow(zscores, 4.0)) - 3.0  # excess kurtosis, should be 0 for Gaussian\n",
    "    final = torch.cat((mean.reshape(1,), std.reshape(1,), skews.reshape(1,), kurtoses.reshape(1,)))\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decorate_with_diffs(data, exponent, remove_raw_data=False):\n",
    "\n",
    "    mean = torch.mean(data.data, 1, keepdim=True)\n",
    "\n",
    "    mean_broadcast = torch.mul(torch.ones(data.size()), mean.tolist()[0][0]).cuda()\n",
    "    if use_gpu:\n",
    "        mean_broadcast = mean_broadcast.cuda()\n",
    "    diffs = torch.pow(data - Variable(mean_broadcast), exponent)\n",
    "    if remove_raw_data:\n",
    "        return torch.cat([diffs], 1)\n",
    "    else:\n",
    "        return torch.cat([data, diffs], 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model parameters\n",
    "\n",
    "num_runs = 10\n",
    "\n",
    "#job_name = \"./saves/baseline/starter_gan_default\"\n",
    "\n",
    "\n",
    "g_input_size = 1     # Random noise dimension coming into generator, per output vector\n",
    "g_hidden_size = 50   # Generator complexity\n",
    "g_output_size = 1    # size of generated output vector\n",
    "d_input_size = 100   # Minibatch size - cardinality of distributions\n",
    "d_hidden_size = 50   # Discriminator complexity\n",
    "d_output_size = 1    # Single dimension for 'real' vs. 'fake'\n",
    "minibatch_size = d_input_size\n",
    "d_learning_rate = 2e-4  # 2e-4\n",
    "g_learning_rate = 2e-4\n",
    "# optim_betas = (0.9, 0.999)\n",
    "num_epochs = 30000\n",
    "print_interval = 200\n",
    "save_interval = 200\n",
    "# d_steps = 1  # 'k' steps in the original GAN paper. Can put the discriminator on higher training freq than generator\n",
    "d_steps = 5\n",
    "g_steps = 1\n",
    "\n",
    "dfe, dre, ge = 0, 0, 0\n",
    "d_real_data, d_fake_data, g_fake_data = None, None, None\n",
    "\n",
    "discriminator_activation_function = torch.sigmoid\n",
    "generator_activation_function = torch.tanh\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(job_name, data_name, data):\n",
    "    #print(\"Plotting the \", data_name, \"...\")\n",
    "    #print(\" Values: %s\" % (str(gen_data)))\n",
    "    fig = plt.hist(data, bins=50)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Histogram of ' + data_name + ' Distribution')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(job_name + data_name + '.png')\n",
    "    #plt.show()\n",
    "    \n",
    "   \n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(job_name):\n",
    "    #print(job_name)\n",
    "    d_sampler = get_distribution_sampler(data_mean, data_stddev)\n",
    "    gi_sampler = get_generator_input_sampler()\n",
    "    G = Generator(input_size=g_input_size, hidden_size=g_hidden_size, output_size=g_output_size)\n",
    "    D = Discriminator(input_size=d_input_func(d_input_size), hidden_size=d_hidden_size, output_size=d_output_size)\n",
    "    # criterion = nn.BCELoss()  # Binary cross entropy: http://pytorch.org/docs/nn.html#bceloss\n",
    "    # d_optimizer = optim.Adam(D.parameters(), lr=d_learning_rate, betas=optim_betas)\n",
    "    # g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate, betas=optim_betas)\n",
    "    d_optimizer = optim.RMSprop(D.parameters(), lr=d_learning_rate)\n",
    "    g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate)\n",
    "\n",
    "\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    #print(\"use_gpu: \", use_gpu)\n",
    "    if use_gpu:\n",
    "        G = G.cuda()\n",
    "        D = D.cuda()\n",
    "    start_time = time.time()\n",
    "    if not os.path.exists(job_name + '_info_over_epoch.txt'):\n",
    "        with open(job_name + '_info_over_epoch.txt', 'a'): pass\n",
    "    open_file = open(job_name + '_info_over_epoch.txt', 'r+')\n",
    "    open_file.seek(0)\n",
    "    open_file.truncate()\n",
    "    open_file.close()\n",
    "    \n",
    "    if not os.path.exists(job_name + '_results.txt'):\n",
    "        with open(job_name + '_results.txt', 'a'): pass\n",
    "    open_file = open(job_name + '_results.txt', 'r+')\n",
    "    open_file.seek(0)\n",
    "    open_file.truncate()\n",
    "    open_file.close()\n",
    "    \n",
    "    avg_dre = 0\n",
    "    avg_dfe = 0\n",
    "    avg_ge = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for d_index in range(d_steps):\n",
    "            # 1. Train D on real+fake\n",
    "            D.zero_grad()\n",
    "            #  1A: Train D on real\n",
    "            d_real_data = Variable(d_sampler(d_input_size))\n",
    "            if use_gpu:\n",
    "                d_real_data = d_real_data.cuda()\n",
    "            \n",
    "            d_real_decision = D(preprocess(d_real_data))\n",
    "            # d_real_error = criterion(d_real_decision, Variable(torch.ones(1)))  # ones = true\n",
    "            d_real_error = -torch.mean(d_real_decision)\n",
    "            d_real_error.backward() # compute/store gradients, but don't change params\n",
    "            #  1B: Train D on fake\n",
    "            d_gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n",
    "            if use_gpu:\n",
    "                d_gen_input = d_gen_input.cuda()\n",
    "            d_fake_data = G(d_gen_input).detach()  # detach to avoid training G on these labels\n",
    "            d_fake_decision = D(preprocess(d_fake_data.t()))\n",
    "            # d_fake_error = criterion(d_fake_decision, Variable(torch.zeros(1)))  # zeros = fake\n",
    "            d_fake_error = torch.mean(d_fake_decision)\n",
    "            d_fake_error.backward()\n",
    "            d_optimizer.step()     # Only optimizes D's parameters; changes based on stored gradients from backward()\n",
    "            dre, dfe = d_real_decision.data[0], d_fake_decision.data[0]\n",
    "            avg_dre += dre\n",
    "            avg_dfe += dfe\n",
    "            # Weight Clipping\n",
    "            for p in D.parameters():\n",
    "                p.data.clamp_(-0.01, 0.01)\n",
    "        for g_index in range(g_steps):\n",
    "            # 2. Train G on D's response (but DO NOT train D on these labels)\n",
    "            G.zero_grad()\n",
    "            gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n",
    "            if use_gpu:\n",
    "                gen_input = gen_input.cuda()\n",
    "            g_fake_data = G(gen_input)\n",
    "            dg_fake_decision = D(preprocess(g_fake_data.t()))\n",
    "            # g_error = criterion(dg_fake_decision, Variable(torch.ones(1)))  # we want to fool, so pretend it's all genuine\n",
    "            g_error = -torch.mean(dg_fake_decision)\n",
    "            g_error.backward()\n",
    "            g_optimizer.step()  # Only optimizes G's parameters\n",
    "            ge = dg_fake_decision.data[0]\n",
    "            avg_ge += ge\n",
    "        \n",
    "        if epoch % save_interval == 0:\n",
    "            with open(job_name + '_info_over_epoch.txt', 'a') as f:\n",
    "                f.write(\"{} {} {} {} {} {}\\n\".format(epoch, avg_dre/(d_steps*save_interval), avg_dfe/(d_steps*save_interval), avg_ge/(g_steps*save_interval), stats(extract(d_real_data)), stats(extract(d_fake_data))))\n",
    "            avg_dre = 0\n",
    "            avg_dfe = 0\n",
    "            avg_ge = 0\n",
    "\n",
    "    #Plot real and generated figures\n",
    "    if matplotlib_is_available:\n",
    "        plot_distribution(job_name, 'Real', d_real_data.cpu())\n",
    "        plot_distribution(job_name, 'Generated', extract(g_fake_data))\n",
    "        \n",
    "    #Calculate and save KL_DIV\n",
    "    input_data = g_fake_data.detach().cpu()\n",
    "    target_data = d_real_data.view(100, 1).cpu()\n",
    "    min_elem = abs(min(torch.min(input_data), torch.min(target_data))) + 1\n",
    "    #print(min_elem)\n",
    "    #min_tensor = Variable(torch.Tensor([min_elem])).expand(input_data.size())\n",
    "    #print(min_tensor)\n",
    "    #print(min_tensor.size())\n",
    "\n",
    "    kl_loss = torch.nn.functional.kl_div(torch.log(input_data + min_elem), target_data + min_elem)\n",
    "    gen_mean = extract(torch.mean(input_data, dim = 0))\n",
    "    gen_std = extract(torch.std(input_data, dim = 0))\n",
    "    real_mean = extract(torch.mean(target_data, dim = 0))\n",
    "    real_std = extract(torch.std(target_data, dim = 0))\n",
    "    \n",
    "    \n",
    "    with open(job_name + '_results.txt', 'a') as f:\n",
    "                f.write(\"\"\"Generated Mean: {} \\nGenerated StdDev: {} \\nReal Mean: {} \\nReal StdDev: {}\\nKL Loss: {} \\n\"\"\".format(gen_mean, gen_std, real_mean, real_std, kl_loss))\n",
    "                f.close()\n",
    "        \n",
    "    '''torch.save({\n",
    "                'model_state_dict': G.state_dict(),\n",
    "                'optimizer_state_dict': g_optimizer.state_dict()\n",
    "                }, job_name + '_generator_model')'''\n",
    "    \n",
    "    print(\"Total run time: \", time.time() - start_time)\n",
    "    \n",
    "    return gen_mean[0], gen_std[0], real_mean[0], real_std[0], kl_loss\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: './saves/WL_tomaxent/WL_0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-9285659fd2b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_runs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnew_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mcurr_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mgen_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: './saves/WL_tomaxent/WL_0'"
     ]
    }
   ],
   "source": [
    "avg_gen_mean, avg_gen_std, avg_real_mean, avg_real_std, avg_kl_loss = 0, 0 ,0 ,0 ,0\n",
    "path_name = \"./saves/WL_tomaxent/WL_\"\n",
    "job_name = \"/05_30_GAN_\"\n",
    "start_time = time.time()\n",
    "for i in range(num_runs):\n",
    "    new_path = path_name + str(i)\n",
    "    os.mkdir(new_path)\n",
    "    curr_job = new_path+job_name\n",
    "    gen_mean, gen_std, real_mean, real_std, kl_loss = train(curr_job)\n",
    "    print(f'--------- Run #{i} --------------')\n",
    "    print(f\"Generated Mean: {gen_mean}\\nGenerated StdDev: {gen_std}\\nReal_Mean: {real_mean}\\nReal StdDev: {real_std}\\nKL_Loss: {kl_loss}\")\n",
    "    \n",
    "    avg_gen_mean += gen_mean\n",
    "    avg_gen_std += gen_std\n",
    "    avg_real_mean += real_mean\n",
    "    avg_real_std += real_std\n",
    "    avg_kl_loss += kl_loss\n",
    "avg_gen_mean /= num_runs\n",
    "avg_gen_std /= num_runs\n",
    "avg_real_mean /= num_runs\n",
    "avg_real_std /= num_runs\n",
    "avg_kl_loss /= num_runs\n",
    "\n",
    "final_data = \"./saves/WL_final_results.txt\"\n",
    "'''if not os.path.exists(final_data):\n",
    "    with open(final_data, 'a'): pass\n",
    "open_file = open(final_data, 'r+')\n",
    "open_file.seek(0)\n",
    "open_file.truncate()\n",
    "open_file.close()\n",
    "'''\n",
    "with open(final_data, 'a') as f:\n",
    "    f.write(\"\"\"Generated Mean: {} \\nGenerated StdDev: {} \\nReal Mean: {} \\nReal StdDev: {}\\nKL Loss: {} \\n\"\"\".format(avg_gen_mean, avg_gen_std, avg_real_mean, avg_real_std, avg_kl_loss))\n",
    "    f.close()\n",
    "print(\"Total run time: \", time.time() - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.8913)\n"
     ]
    }
   ],
   "source": [
    "'''checkpoint=None\n",
    "if os.path.isfile(job_name + '_generator_model'):\n",
    "    print(\"fetching model\")\n",
    "    checkpoint = torch.load(job_name + '_generator_model')\n",
    "    G.load_state_dict(checkpoint['model_state_dict'])\n",
    "    g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate, betas=(beta1, 0.999))\n",
    "    g_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])'''\n",
    "\n",
    "'''G.eval().cpu()\n",
    "with torch.no_grad():\n",
    "    gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n",
    "\n",
    "    g_fake_data = G(gen_input)'''\n",
    "\n",
    "\n",
    "#plot_distribution('Generated', extract(g_fake_data))\n",
    "\n",
    "d_sampler = get_distribution_sampler(data_mean, data_stddev)\n",
    "\n",
    "d_real_data = Variable(d_sampler(d_input_size))\n",
    "x = torch.min(d_real_data)\n",
    "print(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sampler = get_distribution_sampler(100, 1)\n",
    "input_1 = Variable(test_sampler(d_input_size))\n",
    "input_2 = Variable(test_sampler(d_input_size))\n",
    "\n",
    "kl_loss = torch.nn.functional.kl_div(torch.log(input_1), input_2)\n",
    "\n",
    "print(kl_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data [Only 4 moments]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:126: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'autograd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-bc399c194595>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mgradient_penalty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_gradient_penalty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_real_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_fake_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0mgradient_penalty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mD_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# Only optimizes D's parameters; changes based on stored gradients from backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-bc399c194595>\u001b[0m in \u001b[0;36mcalc_gradient_penalty\u001b[0;34m(netD, real_data, fake_data)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mdisc_interpolates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterpolates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n\u001b[0m\u001b[1;32m    100\u001b[0m                               \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_interpolates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                               create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'autograd' is not defined"
     ]
    }
   ],
   "source": [
    "data_mean = 10\n",
    "data_stddev = 5\n",
    "# Model params\n",
    "g_input_size = 1     # Random noise dimension coming into generator, per output vector\n",
    "g_hidden_size = 50   # Generator complexity\n",
    "g_output_size = 1    # size of generated output vector\n",
    "d_input_size = 100   # Minibatch size - cardinality of distributions\n",
    "d_hidden_size = 50   # Discriminator complexity\n",
    "d_output_size = 1    # Single dimension for 'real' vs. 'fake'\n",
    "minibatch_size = d_input_size\n",
    "d_learning_rate = 2e-4  # 2e-4\n",
    "g_learning_rate = 2e-4\n",
    "# optim_betas = (0.9, 0.999)\n",
    "num_epochs = 30000\n",
    "print_interval = 200\n",
    "# d_steps = 1  # 'k' steps in the original GAN paper. Can put the discriminator on higher training freq than generator\n",
    "d_steps = 5\n",
    "g_steps = 1\n",
    "# ### Uncomment only one of these\n",
    "#(name, preprocess, d_input_func) = (\"Raw data\", lambda data: data, lambda x: x)\n",
    "#(name, preprocess, d_input_func) = (\"Data and variances\", lambda data: decorate_with_diffs(data, 2.0), lambda x: x * 2)\n",
    "print(\"Using data [%s]\" % (name))\n",
    "# ##### DATA: Target data and generator input data\n",
    "def get_distribution_sampler(mu, sigma):\n",
    "    return lambda n: torch.Tensor(np.random.normal(mu, sigma, (1, n)))  # Gaussian\n",
    "def get_generator_input_sampler():\n",
    "    return lambda m, n: torch.rand(m, n)  # Uniform-dist data into generator, _NOT_ Gaussian\n",
    "# ##### MODELS: Generator model and discriminator model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.map1(x))\n",
    "        x = F.sigmoid(self.map2(x))\n",
    "        return self.map3(x)\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.map1(x))\n",
    "        x = F.elu(self.map2(x))\n",
    "        # return F.sigmoid(self.map3(x))\n",
    "        return self.map3(x)\n",
    "def extract(v):\n",
    "    return v.data.storage().tolist()\n",
    "def stats(d):\n",
    "    return [np.mean(d), np.std(d)]\n",
    "def decorate_with_diffs(data, exponent):\n",
    "    mean = torch.mean(data.data, 1)\n",
    "    mean_broadcast = torch.mul(torch.ones(data.size()), mean.tolist()[0][0])\n",
    "    diffs = torch.pow(data - Variable(mean_broadcast), exponent)\n",
    "    return torch.cat([data, diffs], 1)\n",
    "d_sampler = get_distribution_sampler(data_mean, data_stddev)\n",
    "gi_sampler = get_generator_input_sampler()\n",
    "G = Generator(input_size=g_input_size, hidden_size=g_hidden_size, output_size=g_output_size)\n",
    "D = Discriminator(input_size=d_input_func(d_input_size), hidden_size=d_hidden_size, output_size=d_output_size)\n",
    "# criterion = nn.BCELoss()  # Binary cross entropy: http://pytorch.org/docs/nn.html#bceloss\n",
    "# d_optimizer = optim.Adam(D.parameters(), lr=d_learning_rate, betas=optim_betas)\n",
    "# g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate, betas=optim_betas)\n",
    "#d_optimizer = optim.RMSprop(D.parameters(), lr=d_learning_rate)\n",
    "#g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate)\n",
    "lr = 1e-4\n",
    "betas = (.9, .99)\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=lr, betas=betas)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "#if use_cuda:\n",
    "    #one = one.cuda()\n",
    "    #mone = mone.cuda()\n",
    "    \n",
    "LAMBDA = .1  # Smaller lambda seems to help for toy tasks specifically\n",
    "    \n",
    "def calc_gradient_penalty(netD, real_data, fake_data):\n",
    "    alpha = torch.rand(1, minibatch_size)\n",
    "    fake_data = fake_data.view(1, 100)\n",
    "    #print(alpha.size())\n",
    "    #print(real_data.size())\n",
    "    #alpha = alpha.expand(real_data.size())\n",
    "    #print((alpha*real_data).size())\n",
    "    #print(((1 - alpha) * fake_data).size())\n",
    "\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "    #print(interpolates.size())\n",
    "\n",
    "\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "    #print(\"Interpolates size: \", interpolates.size())\n",
    "\n",
    "    disc_interpolates = netD(preprocess(interpolates))\n",
    "\n",
    "    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                              grad_outputs=torch.ones(disc_interpolates.size()),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n",
    "    return gradient_penalty\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for d_index in range(d_steps):\n",
    "        \n",
    "        #  1A: Train D on real\n",
    "        d_real_data = Variable(d_sampler(d_input_size))\n",
    "\n",
    "        \n",
    "        # 1. Train D on real+fake\n",
    "        D.zero_grad()\n",
    "        \n",
    "        d_real_decision = D(preprocess(d_real_data))\n",
    "        # d_real_error = criterion(d_real_decision, Variable(torch.ones(1)))  # ones = true\n",
    "        d_real_error = -torch.mean(d_real_decision)\n",
    "        d_real_error.backward() # compute/store gradients, but don't change params\n",
    "        \n",
    "        \n",
    "        #  1B: Train D on fake\n",
    "        d_gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n",
    "        noisev = Variable(d_gen_input, volatile=True)\n",
    "        \n",
    "        d_fake_data = G(d_gen_input).detach()  # detach to avoid training G on these labels\n",
    "        d_fake_decision = D(preprocess(d_fake_data.t()))\n",
    "        # d_fake_error = criterion(d_fake_decision, Variable(torch.zeros(1)))  # zeros = fake\n",
    "        d_fake_error = torch.mean(d_fake_decision)\n",
    "        d_fake_error.backward()\n",
    "        \n",
    "        \n",
    "        gradient_penalty = calc_gradient_penalty(D, d_real_data.data, d_fake_data.data)\n",
    "        gradient_penalty.backward()\n",
    "        D_optimizer.step()     # Only optimizes D's parameters; changes based on stored gradients from backward()\n",
    "       \n",
    "    for g_index in range(g_steps):\n",
    "        # 2. Train G on D's response (but DO NOT train D on these labels)\n",
    "        G.zero_grad()\n",
    "        gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n",
    "        g_fake_data = G(gen_input)\n",
    "        dg_fake_decision = D(preprocess(g_fake_data.t()))\n",
    "        # g_error = criterion(dg_fake_decision, Variable(torch.ones(1)))  # we want to fool, so pretend it's all genuine\n",
    "        g_error = -torch.mean(dg_fake_decision)\n",
    "        g_error.backward()\n",
    "        G_optimizer.step()  # Only optimizes G's parameters\n",
    "    if epoch % print_interval == 0:\n",
    "        print(\"%s: D: %s/%s G: %s (Real: %s, Fake: %s) \" % (epoch,\n",
    "                                                            extract(d_real_error)[0],\n",
    "                                                            extract(d_fake_error)[0],\n",
    "                                                            extract(g_error)[0],\n",
    "                                                            stats(extract(d_real_data)),\n",
    "                                                            stats(extract(d_fake_data))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
