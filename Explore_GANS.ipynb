{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import os\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib_is_available = True\n",
    "try:\n",
    "  from matplotlib import pyplot as plt\n",
    "except ImportError:\n",
    "  print(\"Will skip plotting; matplotlib is not available.\")\n",
    "  matplotlib_is_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data params\n",
    "data_mean = 10\n",
    "data_stddev = 5\n",
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data [Only 4 moments]\n"
     ]
    }
   ],
   "source": [
    "# ### Uncomment only one of these to define what data is actually sent to the Discriminator\n",
    "#(name, preprocess, d_input_func) = (\"Raw data\", lambda data: data, lambda x: x)\n",
    "#(name, preprocess, d_input_func) = (\"Data and variances\", lambda data: decorate_with_diffs(data, 2.0), lambda x: x * 2)\n",
    "#(name, preprocess, d_input_func) = (\"Data and diffs\", lambda data: decorate_with_diffs(data, 1.0), lambda x: x * 2)\n",
    "(name, preprocess, d_input_func) = (\"Only 4 moments\", lambda data: get_moments(data), lambda x: 4)\n",
    "print(\"Using data [%s]\" % (name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution_sampler(mu, sigma):\n",
    "    return lambda n: torch.Tensor(np.random.normal(mu, sigma, (1, n)))  # Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator_input_sampler():\n",
    "    return lambda m, n: torch.rand(m, n)  # Uniform-dist data into generator, _NOT_ Gaussian\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, f):\n",
    "        super(Generator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.map1(x)\n",
    "        x = self.f(x)\n",
    "        x = self.map2(x)\n",
    "        x = self.f(x)\n",
    "        x = self.map3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, f):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.f(self.map1(x))\n",
    "        x = self.f(self.map2(x))\n",
    "        return self.map3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(v):\n",
    "    return v.data.storage().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(d):\n",
    "    return [np.mean(d), np.std(d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moments(d):\n",
    "    # Return the first 4 moments of the data provided\n",
    "    mean = torch.mean(d)\n",
    "    diffs = d - mean\n",
    "    var = torch.mean(torch.pow(diffs, 2.0))\n",
    "    std = torch.pow(var, 0.5)\n",
    "    zscores = diffs / std\n",
    "    skews = torch.mean(torch.pow(zscores, 3.0))\n",
    "    kurtoses = torch.mean(torch.pow(zscores, 4.0)) - 3.0  # excess kurtosis, should be 0 for Gaussian\n",
    "    final = torch.cat((mean.reshape(1,), std.reshape(1,), skews.reshape(1,), kurtoses.reshape(1,)))\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decorate_with_diffs(data, exponent, remove_raw_data=False):\n",
    "\n",
    "    mean = torch.mean(data.data, 1, keepdim=True)\n",
    "\n",
    "    mean_broadcast = torch.mul(torch.ones(data.size()), mean.tolist()[0][0]).cuda()\n",
    "    if use_gpu:\n",
    "        mean_broadcast = mean_broadcast.cuda()\n",
    "    diffs = torch.pow(data - Variable(mean_broadcast), exponent)\n",
    "    if remove_raw_data:\n",
    "        return torch.cat([diffs], 1)\n",
    "    else:\n",
    "        return torch.cat([data, diffs], 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model parameters\n",
    "\n",
    "num_runs = 10\n",
    "\n",
    "#job_name = \"./saves/baseline/starter_gan_default\"\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "g_input_size = 1      # Random noise dimension coming into generator, per output vector\n",
    "g_hidden_size = 5     # Generator complexity\n",
    "g_output_size = 1     # Size of generated output vector\n",
    "d_input_size = 500    # Minibatch size - cardinality of distributions\n",
    "d_hidden_size = 10    # Discriminator complexity\n",
    "d_output_size = 1     # Single dimension for 'real' vs. 'fake' classification\n",
    "minibatch_size = d_input_size\n",
    "\n",
    "d_learning_rate = 2e-4\n",
    "g_learning_rate = 2e-4\n",
    "sgd_momentum = 0.9\n",
    "beta1 = 0.5\n",
    "optim_betas = (0.9, 0.999)\n",
    "\n",
    "num_epochs = 30000\n",
    "print_interval = 100\n",
    "save_interval = 100\n",
    "d_steps = 5\n",
    "g_steps = 1\n",
    "\n",
    "dfe, dre, ge = 0, 0, 0\n",
    "d_real_data, d_fake_data, g_fake_data = None, None, None\n",
    "\n",
    "discriminator_activation_function = torch.sigmoid\n",
    "generator_activation_function = torch.tanh\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(job_name, data_name, data):\n",
    "    #print(\"Plotting the \", data_name, \"...\")\n",
    "    #print(\" Values: %s\" % (str(gen_data)))\n",
    "    fig = plt.hist(data, bins=50)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Histogram of ' + data_name + ' Distribution')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(job_name + data_name + '.png')\n",
    "    #plt.show()\n",
    "    \n",
    "   \n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(job_name):\n",
    "    #print(job_name)\n",
    "    d_sampler = get_distribution_sampler(data_mean, data_stddev)\n",
    "    gi_sampler = get_generator_input_sampler()\n",
    "    G = Generator(input_size=g_input_size,\n",
    "                  hidden_size=g_hidden_size,\n",
    "                  output_size=g_output_size,\n",
    "                  f=generator_activation_function)\n",
    "    D = Discriminator(input_size=d_input_func(d_input_size),\n",
    "                      hidden_size=d_hidden_size,\n",
    "                      output_size=d_output_size,\n",
    "                      f=discriminator_activation_function)\n",
    "    criterion = nn.BCELoss()  # Binary cross entropy: http://pytorch.org/docs/nn.html#bceloss\n",
    "    #d_optimizer = optim.SGD(D.parameters(), lr=d_learning_rate, momentum=sgd_momentum)\n",
    "    #g_optimizer = optim.SGD(G.parameters(), lr=g_learning_rate, momentum=sgd_momentum)\n",
    "    #d_optimizer = optim.Adam(D.parameters(), lr=d_learning_rate, betas=(beta1, 0.999))\n",
    "    #g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate, betas=(beta1, 0.999))\n",
    "    d_optimizer = optim.RMSprop(D.parameters(), lr = 0.00005)\n",
    "    g_optimizer = optim.RMSprop(G.parameters(), lr = 0.00005)\n",
    "\n",
    "\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    #print(\"use_gpu: \", use_gpu)\n",
    "    if use_gpu:\n",
    "        G = G.cuda()\n",
    "        D = D.cuda()\n",
    "    start_time = time.time()\n",
    "    if not os.path.exists(job_name + '_info_over_epoch.txt'):\n",
    "        with open(job_name + '_info_over_epoch.txt', 'a'): pass\n",
    "    open_file = open(job_name + '_info_over_epoch.txt', 'r+')\n",
    "    open_file.seek(0)\n",
    "    open_file.truncate()\n",
    "    open_file.close()\n",
    "    \n",
    "    if not os.path.exists(job_name + '_results.txt'):\n",
    "        with open(job_name + '_results.txt', 'a'): pass\n",
    "    open_file = open(job_name + '_results.txt', 'r+')\n",
    "    open_file.seek(0)\n",
    "    open_file.truncate()\n",
    "    open_file.close()\n",
    "    \n",
    "    avg_dre = 0\n",
    "    avg_dfe = 0\n",
    "    avg_ge = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        for d_index in range(d_steps):\n",
    "            # 1. Train D on real+fake\n",
    "            D.zero_grad()\n",
    "\n",
    "            #  1A: Train D on real\n",
    "            d_real_data = Variable(d_sampler(d_input_size))\n",
    "            \n",
    "            if use_gpu:\n",
    "                d_real_data = d_real_data.cuda()\n",
    "            \n",
    "            d_real_decision = D(preprocess(d_real_data))\n",
    "\n",
    "            #labels = Variable(torch.ones([1]))\n",
    "            \n",
    "            #if use_gpu:\n",
    "                #labels = labels.cuda()\n",
    "            #d_real_error = criterion(d_real_decision, labels)   # ones = true\n",
    "            #Wassterstein loss\n",
    "            d_real_error = -torch.mean(d_real_decision)\n",
    "            d_real_error.backward() # compute/store gradients, but don't change params\n",
    "\n",
    "            #  1B: Train D on fake\n",
    "            d_gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n",
    "            if use_gpu:\n",
    "                d_gen_input = d_gen_input.cuda()\n",
    "            d_fake_data = G(d_gen_input).detach()  # detach to avoid training G on these labels\n",
    "            d_fake_decision = D(preprocess(d_fake_data.t()))\n",
    "            \n",
    "            #labels = Variable(torch.zeros([1]))             \n",
    "            #if use_gpu:\n",
    "                #labels = labels.cuda()\n",
    "            #d_fake_error = criterion(d_fake_decision, labels)  # zeros = fake\n",
    "            #Wassterstein loss\n",
    "            d_fake_error = torch.mean(d_fake_decision)\n",
    "            d_fake_error.backward()\n",
    "            d_optimizer.step()     # Only optimizes D's parameters; changes based on stored gradients from backward()\n",
    "\n",
    "            dre, dfe = extract(d_real_error)[0], extract(d_fake_error)[0]\n",
    "            avg_dre += dre\n",
    "            avg_dfe += dfe\n",
    "            \n",
    "            #Wasserstein weight clipping \n",
    "            for p in D.parameters():\n",
    "                p.data.clamp_(-0.01, 0.01)\n",
    "            \n",
    "            \n",
    "\n",
    "        for g_index in range(g_steps):\n",
    "            # 2. Train G on D's response (but DO NOT train D on these labels)\n",
    "            G.zero_grad()\n",
    "\n",
    "            gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n",
    "            if use_gpu:\n",
    "                gen_input = gen_input.cuda()\n",
    "                              \n",
    "            g_fake_data = G(gen_input)\n",
    "            dg_fake_decision = D(preprocess(g_fake_data.t()))\n",
    "                              \n",
    "            #labels = Variable(torch.ones([1]))\n",
    "            #if use_gpu:\n",
    "                #labels = labels.cuda()\n",
    "            #g_error = criterion(dg_fake_decision, labels)  # Train G to pretend it's genuine\n",
    "            #Wassterstein loss\n",
    "            g_error = -torch.mean(dg_fake_decision)\n",
    "            g_error.backward()\n",
    "            g_optimizer.step()  # Only optimizes G's parameters\n",
    "            ge = extract(g_error)[0]\n",
    "            avg_ge += ge\n",
    "        \n",
    "       \n",
    "\n",
    "        '''if epoch % print_interval == 0:\n",
    "            print(\"Epoch %s: D (%s real_err, %s fake_err) G (%s err); Real Dist (%s),  Fake Dist (%s) \" %\n",
    "                  (epoch, dre, dfe, ge, stats(extract(d_real_data)), stats(extract(d_fake_data))))'''\n",
    "        if epoch % save_interval == 0:\n",
    "            with open(job_name + '_info_over_epoch.txt', 'a') as f:\n",
    "                f.write(\"{} {} {} {} {} {}\\n\".format(epoch, avg_dre/(d_steps*save_interval), avg_dfe/(d_steps*save_interval), avg_ge/(g_steps*save_interval), stats(extract(d_real_data)), stats(extract(d_fake_data))))\n",
    "            avg_dre = 0\n",
    "            avg_dfe = 0\n",
    "            avg_ge = 0\n",
    "\n",
    "    #Plot real and generated figures\n",
    "    if matplotlib_is_available:\n",
    "        plot_distribution(job_name, 'Real', d_real_data.cpu())\n",
    "        plot_distribution(job_name, 'Generated', extract(g_fake_data))\n",
    "        \n",
    "    #Calculate and save KL_DIV\n",
    "    input_data = g_fake_data.detach().cpu()\n",
    "    target_data = d_real_data.view(500, 1).cpu()\n",
    "    min_elem = abs(min(torch.min(input_data), torch.min(target_data))) + 1\n",
    "    #print(min_elem)\n",
    "    #min_tensor = Variable(torch.Tensor([min_elem])).expand(input_data.size())\n",
    "    #print(min_tensor)\n",
    "    #print(min_tensor.size())\n",
    "\n",
    "    kl_loss = torch.nn.functional.kl_div(torch.log(input_data + min_elem), target_data + min_elem)\n",
    "    gen_mean = extract(torch.mean(input_data, dim = 0))\n",
    "    gen_std = extract(torch.std(input_data, dim = 0))\n",
    "    real_mean = extract(torch.mean(target_data, dim = 0))\n",
    "    real_std = extract(torch.std(target_data, dim = 0))\n",
    "    \n",
    "    \n",
    "    with open(job_name + '_results.txt', 'a') as f:\n",
    "                f.write(\"\"\"Generated Mean: {} \\nGenerated StdDev: {} \\nReal Mean: {} \\nReal StdDev: {}\\nKL Loss: {} \\n\"\"\".format(gen_mean, gen_std, real_mean, real_std, kl_loss))\n",
    "                f.close()\n",
    "        \n",
    "    '''torch.save({\n",
    "                'model_state_dict': G.state_dict(),\n",
    "                'optimizer_state_dict': g_optimizer.state_dict()\n",
    "                }, job_name + '_generator_model')'''\n",
    "    \n",
    "    print(\"Total run time: \", time.time() - start_time)\n",
    "    \n",
    "    return gen_mean[0], gen_std[0], real_mean[0], real_std[0], kl_loss\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1946: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total run time:  833.9913806915283\n",
      "--------- Run #0 --------------\n",
      "Generated Mean: 9.215051651000977\n",
      "Generated StdDev: 0.0006499369046650827\n",
      "Real_Mean: 9.925603866577148\n",
      "Real StdDev: 4.791837692260742\n",
      "KL_Loss: 1.5389875173568726\n",
      "Total run time:  849.1959466934204\n",
      "--------- Run #1 --------------\n",
      "Generated Mean: 4.522304058074951\n",
      "Generated StdDev: 0.0032297391444444656\n",
      "Real_Mean: 9.795001029968262\n",
      "Real StdDev: 5.145484447479248\n",
      "KL_Loss: 7.036130905151367\n",
      "Total run time:  842.2988328933716\n",
      "--------- Run #2 --------------\n",
      "Generated Mean: 9.870133399963379\n",
      "Generated StdDev: 0.007295706309378147\n",
      "Real_Mean: 10.276829719543457\n",
      "Real StdDev: 4.895864963531494\n",
      "KL_Loss: 1.270961880683899\n",
      "Total run time:  750.5670893192291\n",
      "--------- Run #3 --------------\n",
      "Generated Mean: 9.737217903137207\n",
      "Generated StdDev: 0.00011958429968217388\n",
      "Real_Mean: 10.21662712097168\n",
      "Real StdDev: 4.983131408691406\n",
      "KL_Loss: 1.375558853149414\n"
     ]
    }
   ],
   "source": [
    "avg_gen_mean, avg_gen_std, avg_real_mean, avg_real_std, avg_kl_loss = 0, 0 ,0 ,0 ,0\n",
    "path_name = \"./saves/WL_tomaxent/WL_\"\n",
    "job_name = \"/05_30_GAN_\"\n",
    "start_time = time.time()\n",
    "for i in range(num_runs):\n",
    "    new_path = path_name + str(i)\n",
    "    os.mkdir(new_path)\n",
    "    curr_job = new_path+job_name\n",
    "    gen_mean, gen_std, real_mean, real_std, kl_loss = train(curr_job)\n",
    "    print(f'--------- Run #{i} --------------')\n",
    "    print(f\"Generated Mean: {gen_mean}\\nGenerated StdDev: {gen_std}\\nReal_Mean: {real_mean}\\nReal StdDev: {real_std}\\nKL_Loss: {kl_loss}\")\n",
    "    \n",
    "    avg_gen_mean += gen_mean\n",
    "    avg_gen_std += gen_std\n",
    "    avg_real_mean += real_mean\n",
    "    avg_real_std += real_std\n",
    "    avg_kl_loss += kl_loss\n",
    "avg_gen_mean /= num_runs\n",
    "avg_gen_std /= num_runs\n",
    "avg_real_mean /= num_runs\n",
    "avg_real_std /= num_runs\n",
    "avg_kl_loss /= num_runs\n",
    "\n",
    "final_data = \"./saves/WL_final_results.txt\"\n",
    "'''if not os.path.exists(final_data):\n",
    "    with open(final_data, 'a'): pass\n",
    "open_file = open(final_data, 'r+')\n",
    "open_file.seek(0)\n",
    "open_file.truncate()\n",
    "open_file.close()\n",
    "'''\n",
    "with open(final_data, 'a') as f:\n",
    "    f.write(\"\"\"Generated Mean: {} \\nGenerated StdDev: {} \\nReal Mean: {} \\nReal StdDev: {}\\nKL Loss: {} \\n\"\"\".format(avg_gen_mean, avg_gen_std, avg_real_mean, avg_real_std, avg_kl_loss))\n",
    "    f.close()\n",
    "print(\"Total run time: \", time.time() - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''checkpoint=None\n",
    "if os.path.isfile(job_name + '_generator_model'):\n",
    "    print(\"fetching model\")\n",
    "    checkpoint = torch.load(job_name + '_generator_model')\n",
    "    G.load_state_dict(checkpoint['model_state_dict'])\n",
    "    g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate, betas=(beta1, 0.999))\n",
    "    g_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])'''\n",
    "\n",
    "'''G.eval().cpu()\n",
    "with torch.no_grad():\n",
    "    gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n",
    "\n",
    "    g_fake_data = G(gen_input)'''\n",
    "\n",
    "\n",
    "#plot_distribution('Generated', extract(g_fake_data))\n",
    "\n",
    "d_sampler = get_distribution_sampler(data_mean, data_stddev)\n",
    "\n",
    "d_real_data = Variable(d_sampler(d_input_size))\n",
    "x = torch.min(d_real_data)\n",
    "print(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sampler = get_distribution_sampler(100, 1)\n",
    "input_1 = Variable(test_sampler(d_input_size))\n",
    "input_2 = Variable(test_sampler(d_input_size))\n",
    "\n",
    "kl_loss = torch.nn.functional.kl_div(torch.log(input_1), input_2)\n",
    "\n",
    "print(kl_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
