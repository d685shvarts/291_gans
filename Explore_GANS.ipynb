{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import time\n",
    "import os\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib_is_available = True\n",
    "try:\n",
    "  from matplotlib import pyplot as plt\n",
    "except ImportError:\n",
    "  print(\"Will skip plotting; matplotlib is not available.\")\n",
    "  matplotlib_is_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data params\n",
    "data_mean = 10\n",
    "data_stddev = 5\n",
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data [Only 4 moments]\n"
     ]
    }
   ],
   "source": [
    "# ### Uncomment only one of these to define what data is actually sent to the Discriminator\n",
    "#(name, preprocess, d_input_func) = (\"Raw data\", lambda data: data, lambda x: x)\n",
    "#(name, preprocess, d_input_func) = (\"Data and variances\", lambda data: decorate_with_diffs(data, 2.0), lambda x: x * 2)\n",
    "#(name, preprocess, d_input_func) = (\"Data and diffs\", lambda data: decorate_with_diffs(data, 1.0), lambda x: x * 2)\n",
    "(name, preprocess, d_input_func) = (\"Only 4 moments\", lambda data: get_moments(data), lambda x: 4)\n",
    "print(\"Using data [%s]\" % (name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution_sampler(mu, sigma):\n",
    "    return lambda n: torch.Tensor(np.random.normal(mu, sigma, (1, n)))  # Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator_input_sampler():\n",
    "    return lambda m, n: torch.rand(m, n)  # Uniform-dist data into generator, _NOT_ Gaussian\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.map1(x))\n",
    "        x = F.sigmoid(self.map2(x))\n",
    "        return self.map3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.map1(x))\n",
    "        x = F.elu(self.map2(x))\n",
    "        # return F.sigmoid(self.map3(x))\n",
    "        return self.map3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(v):\n",
    "    return v.data.storage().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(d):\n",
    "    return [np.mean(d), np.std(d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moments(d):\n",
    "    # Return the first 4 moments of the data provided\n",
    "    mean = torch.mean(d)\n",
    "    diffs = d - mean\n",
    "    var = torch.mean(torch.pow(diffs, 2.0))\n",
    "    std = torch.pow(var, 0.5)\n",
    "    zscores = diffs / std\n",
    "    skews = torch.mean(torch.pow(zscores, 3.0))\n",
    "    kurtoses = torch.mean(torch.pow(zscores, 4.0)) - 3.0  # excess kurtosis, should be 0 for Gaussian\n",
    "    final = torch.cat((mean.reshape(1,), std.reshape(1,), skews.reshape(1,), kurtoses.reshape(1,)))\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decorate_with_diffs(data, exponent, remove_raw_data=False):\n",
    "\n",
    "    mean = torch.mean(data.data, 1, keepdim=True)\n",
    "\n",
    "    mean_broadcast = torch.mul(torch.ones(data.size()), mean.tolist()[0][0]).cuda()\n",
    "    if use_gpu:\n",
    "        mean_broadcast = mean_broadcast.cuda()\n",
    "    diffs = torch.pow(data - Variable(mean_broadcast), exponent)\n",
    "    if remove_raw_data:\n",
    "        return torch.cat([diffs], 1)\n",
    "    else:\n",
    "        return torch.cat([data, diffs], 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model parameters\n",
    "\n",
    "num_runs = 10\n",
    "\n",
    "#job_name = \"./saves/baseline/starter_gan_default\"\n",
    "\n",
    "\n",
    "g_input_size = 1     # Random noise dimension coming into generator, per output vector\n",
    "g_hidden_size = 50   # Generator complexity\n",
    "g_output_size = 1    # size of generated output vector\n",
    "d_input_size = 100   # Minibatch size - cardinality of distributions\n",
    "d_hidden_size = 50   # Discriminator complexity\n",
    "d_output_size = 1    # Single dimension for 'real' vs. 'fake'\n",
    "minibatch_size = d_input_size\n",
    "d_learning_rate = 2e-4  # 2e-4\n",
    "g_learning_rate = 2e-4\n",
    "# optim_betas = (0.9, 0.999)\n",
    "num_epochs = 30000\n",
    "print_interval = 200\n",
    "save_interval = 200\n",
    "# d_steps = 1  # 'k' steps in the original GAN paper. Can put the discriminator on higher training freq than generator\n",
    "d_steps = 5\n",
    "g_steps = 1\n",
    "\n",
    "dfe, dre, ge = 0, 0, 0\n",
    "d_real_data, d_fake_data, g_fake_data = None, None, None\n",
    "\n",
    "discriminator_activation_function = torch.sigmoid\n",
    "generator_activation_function = torch.tanh\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(job_name, data_name, data):\n",
    "    #print(\"Plotting the \", data_name, \"...\")\n",
    "    #print(\" Values: %s\" % (str(gen_data)))\n",
    "    fig = plt.hist(data, bins=50)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Histogram of ' + data_name + ' Distribution')\n",
    "    plt.grid(True)\n",
    "    #plt.savefig(job_name + data_name + '.png')\n",
    "    plt.show()\n",
    "    \n",
    "   \n",
    "    #plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(job_name):\n",
    "    #print(job_name)\n",
    "    d_sampler = get_distribution_sampler(data_mean, data_stddev)\n",
    "    gi_sampler = get_generator_input_sampler()\n",
    "    G = Generator(input_size=g_input_size, hidden_size=g_hidden_size, output_size=g_output_size)\n",
    "    D = Discriminator(input_size=d_input_func(d_input_size), hidden_size=d_hidden_size, output_size=d_output_size)\n",
    "    # criterion = nn.BCELoss()  # Binary cross entropy: http://pytorch.org/docs/nn.html#bceloss\n",
    "    # d_optimizer = optim.Adam(D.parameters(), lr=d_learning_rate, betas=optim_betas)\n",
    "    # g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate, betas=optim_betas)\n",
    "    d_optimizer = optim.RMSprop(D.parameters(), lr=d_learning_rate)\n",
    "    g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate)\n",
    "\n",
    "\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    #print(\"use_gpu: \", use_gpu)\n",
    "    if use_gpu:\n",
    "        G = G.cuda()\n",
    "        D = D.cuda()\n",
    "    start_time = time.time()\n",
    "    if not os.path.exists(job_name + '_info_over_epoch.txt'):\n",
    "        with open(job_name + '_info_over_epoch.txt', 'a'): pass\n",
    "    open_file = open(job_name + '_info_over_epoch.txt', 'r+')\n",
    "    open_file.seek(0)\n",
    "    open_file.truncate()\n",
    "    open_file.close()\n",
    "    \n",
    "    if not os.path.exists(job_name + '_results.txt'):\n",
    "        with open(job_name + '_results.txt', 'a'): pass\n",
    "    open_file = open(job_name + '_results.txt', 'r+')\n",
    "    open_file.seek(0)\n",
    "    open_file.truncate()\n",
    "    open_file.close()\n",
    "    \n",
    "    avg_dre = 0\n",
    "    avg_dfe = 0\n",
    "    avg_ge = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for d_index in range(d_steps):\n",
    "            # 1. Train D on real+fake\n",
    "            D.zero_grad()\n",
    "            #  1A: Train D on real\n",
    "            d_real_data = Variable(d_sampler(d_input_size))\n",
    "            if use_gpu:\n",
    "                d_real_data = d_real_data.cuda()\n",
    "            \n",
    "            d_real_decision = D(preprocess(d_real_data))\n",
    "            # d_real_error = criterion(d_real_decision, Variable(torch.ones(1)))  # ones = true\n",
    "            d_real_error = -torch.mean(d_real_decision)\n",
    "            d_real_error.backward() # compute/store gradients, but don't change params\n",
    "            #  1B: Train D on fake\n",
    "            d_gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n",
    "            if use_gpu:\n",
    "                d_gen_input = d_gen_input.cuda()\n",
    "            d_fake_data = G(d_gen_input).detach()  # detach to avoid training G on these labels\n",
    "            d_fake_decision = D(preprocess(d_fake_data.t()))\n",
    "            # d_fake_error = criterion(d_fake_decision, Variable(torch.zeros(1)))  # zeros = fake\n",
    "            d_fake_error = torch.mean(d_fake_decision)\n",
    "            d_fake_error.backward()\n",
    "            d_optimizer.step()     # Only optimizes D's parameters; changes based on stored gradients from backward()\n",
    "            dre, dfe = d_real_decision.data[0], d_fake_decision.data[0]\n",
    "            avg_dre += dre\n",
    "            avg_dfe += dfe\n",
    "            # Weight Clipping\n",
    "            for p in D.parameters():\n",
    "                p.data.clamp_(-0.01, 0.01)\n",
    "        for g_index in range(g_steps):\n",
    "            # 2. Train G on D's response (but DO NOT train D on these labels)\n",
    "            G.zero_grad()\n",
    "            gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n",
    "            if use_gpu:\n",
    "                gen_input = gen_input.cuda()\n",
    "            g_fake_data = G(gen_input)\n",
    "            dg_fake_decision = D(preprocess(g_fake_data.t()))\n",
    "            # g_error = criterion(dg_fake_decision, Variable(torch.ones(1)))  # we want to fool, so pretend it's all genuine\n",
    "            g_error = -torch.mean(dg_fake_decision)\n",
    "            g_error.backward()\n",
    "            g_optimizer.step()  # Only optimizes G's parameters\n",
    "            ge = dg_fake_decision.data[0]\n",
    "            avg_ge += ge\n",
    "        \n",
    "        if epoch % save_interval == 0:\n",
    "            with open(job_name + '_info_over_epoch.txt', 'a') as f:\n",
    "                f.write(\"{} {} {} {} {} {}\\n\".format(epoch, avg_dre/(d_steps*save_interval), avg_dfe/(d_steps*save_interval), avg_ge/(g_steps*save_interval), stats(extract(d_real_data)), stats(extract(d_fake_data))))\n",
    "            avg_dre = 0\n",
    "            avg_dfe = 0\n",
    "            avg_ge = 0\n",
    "\n",
    "    #Plot real and generated figures\n",
    "    if matplotlib_is_available:\n",
    "        plot_distribution(job_name, 'Real', d_real_data.cpu())\n",
    "        plot_distribution(job_name, 'Generated', extract(g_fake_data))\n",
    "        \n",
    "    #Calculate and save KL_DIV\n",
    "    input_data = g_fake_data.detach().cpu()\n",
    "    target_data = d_real_data.view(100, 1).cpu()\n",
    "    min_elem = abs(min(torch.min(input_data), torch.min(target_data))) + 1\n",
    "    #print(min_elem)\n",
    "    #min_tensor = Variable(torch.Tensor([min_elem])).expand(input_data.size())\n",
    "    #print(min_tensor)\n",
    "    #print(min_tensor.size())\n",
    "\n",
    "    kl_loss = torch.nn.functional.kl_div(torch.log(input_data + min_elem), target_data + min_elem)\n",
    "    gen_mean = extract(torch.mean(input_data, dim = 0))\n",
    "    gen_std = extract(torch.std(input_data, dim = 0))\n",
    "    real_mean = extract(torch.mean(target_data, dim = 0))\n",
    "    real_std = extract(torch.std(target_data, dim = 0))\n",
    "    \n",
    "    \n",
    "    with open(job_name + '_results.txt', 'a') as f:\n",
    "                f.write(\"\"\"Generated Mean: {} \\nGenerated StdDev: {} \\nReal Mean: {} \\nReal StdDev: {}\\nKL Loss: {} \\n\"\"\".format(gen_mean, gen_std, real_mean, real_std, kl_loss))\n",
    "                f.close()\n",
    "        \n",
    "    '''torch.save({\n",
    "                'model_state_dict': G.state_dict(),\n",
    "                'optimizer_state_dict': g_optimizer.state_dict()\n",
    "                }, job_name + '_generator_model')'''\n",
    "    \n",
    "    print(\"Total run time: \", time.time() - start_time)\n",
    "    \n",
    "    return gen_mean[0], gen_std[0], real_mean[0], real_std[0], kl_loss\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: './saves/WL_tomaxent/WL_0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-9285659fd2b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_runs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnew_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mcurr_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mgen_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: './saves/WL_tomaxent/WL_0'"
     ]
    }
   ],
   "source": [
    "avg_gen_mean, avg_gen_std, avg_real_mean, avg_real_std, avg_kl_loss = 0, 0 ,0 ,0 ,0\n",
    "path_name = \"./saves/WL_tomaxent/WL_\"\n",
    "job_name = \"/05_30_GAN_\"\n",
    "start_time = time.time()\n",
    "for i in range(num_runs):\n",
    "    new_path = path_name + str(i)\n",
    "    os.mkdir(new_path)\n",
    "    curr_job = new_path+job_name\n",
    "    gen_mean, gen_std, real_mean, real_std, kl_loss = train(curr_job)\n",
    "    print(f'--------- Run #{i} --------------')\n",
    "    print(f\"Generated Mean: {gen_mean}\\nGenerated StdDev: {gen_std}\\nReal_Mean: {real_mean}\\nReal StdDev: {real_std}\\nKL_Loss: {kl_loss}\")\n",
    "    \n",
    "    avg_gen_mean += gen_mean\n",
    "    avg_gen_std += gen_std\n",
    "    avg_real_mean += real_mean\n",
    "    avg_real_std += real_std\n",
    "    avg_kl_loss += kl_loss\n",
    "avg_gen_mean /= num_runs\n",
    "avg_gen_std /= num_runs\n",
    "avg_real_mean /= num_runs\n",
    "avg_real_std /= num_runs\n",
    "avg_kl_loss /= num_runs\n",
    "\n",
    "final_data = \"./saves/WL_final_results.txt\"\n",
    "'''if not os.path.exists(final_data):\n",
    "    with open(final_data, 'a'): pass\n",
    "open_file = open(final_data, 'r+')\n",
    "open_file.seek(0)\n",
    "open_file.truncate()\n",
    "open_file.close()\n",
    "'''\n",
    "with open(final_data, 'a') as f:\n",
    "    f.write(\"\"\"Generated Mean: {} \\nGenerated StdDev: {} \\nReal Mean: {} \\nReal StdDev: {}\\nKL Loss: {} \\n\"\"\".format(avg_gen_mean, avg_gen_std, avg_real_mean, avg_real_std, avg_kl_loss))\n",
    "    f.close()\n",
    "print(\"Total run time: \", time.time() - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.8913)\n"
     ]
    }
   ],
   "source": [
    "'''checkpoint=None\n",
    "if os.path.isfile(job_name + '_generator_model'):\n",
    "    print(\"fetching model\")\n",
    "    checkpoint = torch.load(job_name + '_generator_model')\n",
    "    G.load_state_dict(checkpoint['model_state_dict'])\n",
    "    g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate, betas=(beta1, 0.999))\n",
    "    g_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])'''\n",
    "\n",
    "'''G.eval().cpu()\n",
    "with torch.no_grad():\n",
    "    gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n",
    "\n",
    "    g_fake_data = G(gen_input)'''\n",
    "\n",
    "\n",
    "#plot_distribution('Generated', extract(g_fake_data))\n",
    "\n",
    "d_sampler = get_distribution_sampler(data_mean, data_stddev)\n",
    "\n",
    "d_real_data = Variable(d_sampler(d_input_size))\n",
    "x = torch.min(d_real_data)\n",
    "print(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sampler = get_distribution_sampler(100, 1)\n",
    "input_1 = Variable(test_sampler(d_input_size))\n",
    "input_2 = Variable(test_sampler(d_input_size))\n",
    "\n",
    "kl_loss = torch.nn.functional.kl_div(torch.log(input_1), input_2)\n",
    "\n",
    "print(kl_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data [Only 4 moments]\n",
      "0: D: -0.6192820072174072/0.4124716520309448 G: -0.4157402515411377 (Real: [9.662786051034928, 5.083345870989862], Fake: [0.416041356921196, 0.0002415919145685649]) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:126: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200: D: -17.70221710205078/0.32307878136634827 G: -0.3288811445236206 (Real: [9.790859960913657, 4.714276550878154], Fake: [0.5091559797525406, 0.0023915768024622024]) \n",
      "400: D: -55.18533706665039/-4.113702297210693 G: 3.8627030849456787 (Real: [9.753457130491734, 5.1475163237831305], Fake: [0.6164691197872162, 0.003918830354412939]) \n",
      "600: D: -122.57506561279297/-11.879233360290527 G: 12.546199798583984 (Real: [9.479424178898334, 5.612711930947127], Fake: [0.8557269984483719, 0.005899816017178743]) \n",
      "800: D: -218.66624450683594/-18.52759552001953 G: 20.546100616455078 (Real: [9.86254470050335, 4.637921983996538], Fake: [1.2335774099826813, 0.005960086944045227]) \n",
      "1000: D: -384.3892517089844/-25.36939239501953 G: 17.203201293945312 (Real: [10.520494924783707, 5.724990121503178], Fake: [1.325110809803009, 0.0008902662594323366]) \n",
      "1200: D: -542.8588256835938/-32.019866943359375 G: 33.53192138671875 (Real: [9.718663757145405, 4.8713512639980046], Fake: [1.3403606808185577, 0.0015697920421282735]) \n",
      "1400: D: -753.6356201171875/-65.58011627197266 G: 70.82939147949219 (Real: [9.49754922106862, 4.495035954122902], Fake: [1.377129031419754, 0.0030406084625510203]) \n",
      "1600: D: -1132.8978271484375/-87.2551498413086 G: 96.19505310058594 (Real: [9.776625163927674, 5.639288469351876], Fake: [1.4280960083007812, 0.003487182840299546]) \n",
      "1800: D: -1425.3218994140625/105.35929107666016 G: -154.55502319335938 (Real: [10.128624895736575, 4.579876456259195], Fake: [1.4603707277774811, 0.00040780644220225396]) \n",
      "2000: D: -1855.4619140625/109.00044250488281 G: -316.3647155761719 (Real: [10.257314400672913, 4.160030430364451], Fake: [1.4699095869064331, 0.00024187666014793008]) \n",
      "2200: D: -2316.5009765625/293.0608215332031 G: -187.1602020263672 (Real: [9.550143684446812, 4.69075227811902], Fake: [1.4883103251457215, 0.00015922359351771293]) \n",
      "2400: D: -2837.847900390625/88.3757553100586 G: -39.3000373840332 (Real: [9.457863378524781, 4.450792848864093], Fake: [1.5113230919837952, 0.0001712480600694765]) \n",
      "2600: D: -3889.479736328125/-50.52701950073242 G: 81.73706817626953 (Real: [10.514016017243266, 5.13091333077412], Fake: [1.5442188012599944, 0.00021524418011390574]) \n",
      "2800: D: -4511.81982421875/23.098514556884766 G: -20.46181297302246 (Real: [9.455328205525875, 5.480448474012496], Fake: [1.571780159473419, 0.00037285107874433554]) \n",
      "3000: D: -5542.11865234375/-54.42416763305664 G: 48.871768951416016 (Real: [9.707910542748868, 5.62022190719938], Fake: [1.6382824850082398, 0.0004050341629040517]) \n",
      "3200: D: -6554.8798828125/-85.06817626953125 G: 88.35664367675781 (Real: [10.496889161169529, 4.729415163679717], Fake: [1.7722892701625823, 0.001466461222384054]) \n",
      "3400: D: -7339.3349609375/-90.33898162841797 G: 94.5750503540039 (Real: [9.917793056368827, 4.561296654596329], Fake: [2.0229815363883974, 0.006989922440175649]) \n",
      "3600: D: -8605.1630859375/-15.018095970153809 G: 13.232295036315918 (Real: [10.283717423826456, 4.409033832218552], Fake: [2.4015436959266663, 0.019641283744812284]) \n",
      "3800: D: -9509.1328125/141.7256622314453 G: -120.87236022949219 (Real: [9.697447818517684, 4.3999585417289975], Fake: [2.901618633270264, 0.05529247549076631]) \n",
      "4000: D: -11995.07421875/470.9858093261719 G: -496.5770568847656 (Real: [10.564352497458458, 4.9389562295700395], Fake: [3.473686258792877, 0.10615264335148204]) \n",
      "4200: D: -13085.703125/1091.3917236328125 G: -971.0653076171875 (Real: [10.011989453434945, 4.96156650028684], Fake: [4.044355382919312, 0.14591812267884632]) \n",
      "4400: D: -15820.8154296875/1578.9813232421875 G: -1531.9156494140625 (Real: [10.148420796990395, 6.156010359454463], Fake: [4.644731087684631, 0.22075135327416823]) \n",
      "4600: D: -15396.41796875/2317.40576171875 G: -2260.642822265625 (Real: [9.65542162835598, 4.645905291954695], Fake: [5.2163769721984865, 0.27232760297527125]) \n",
      "4800: D: -18473.30859375/3135.661376953125 G: -3222.039306640625 (Real: [10.601588193774223, 5.067430058864119], Fake: [5.784854884147644, 0.32906216978923314]) \n",
      "5000: D: -18714.12890625/4062.3564453125 G: -4012.26318359375 (Real: [9.624421208798886, 4.972018178462487], Fake: [6.337735390663147, 0.3415362577844963]) \n",
      "5200: D: -19638.92578125/5262.603515625 G: -5074.85205078125 (Real: [9.723867318928242, 4.494965594973747], Fake: [6.88276394367218, 0.40361594546491397]) \n",
      "5400: D: -21671.671875/5963.826171875 G: -5968.19091796875 (Real: [9.783181112855672, 4.999431538479532], Fake: [7.119067997932434, 0.5729935906352814]) \n",
      "5600: D: -25106.21484375/6963.7138671875 G: -7178.7216796875 (Real: [11.502575640678407, 4.9644735845333505], Fake: [7.5631186962127686, 0.6294513521861973]) \n",
      "5800: D: -23722.23828125/7771.95947265625 G: -7972.9248046875 (Real: [9.836005165055393, 5.347123857821593], Fake: [7.8659137344360355, 0.7753817013373873]) \n",
      "6000: D: -22997.30078125/8803.140625 G: -9211.1083984375 (Real: [9.882314101457595, 4.8012456534252905], Fake: [8.263418760299682, 0.9404507881484973]) \n",
      "6200: D: -23855.478515625/9550.369140625 G: -9545.482421875 (Real: [10.30960694503039, 5.1090014619586], Fake: [8.531500005722046, 1.1630429411691088]) \n",
      "6400: D: -22327.958984375/10439.09765625 G: -9969.60546875 (Real: [9.568764661317692, 5.1648782741381964], Fake: [8.911454124450684, 1.3184239741533188]) \n",
      "6600: D: -21214.75/11106.59375 G: -11060.38671875 (Real: [9.825514444857836, 4.596211446573848], Fake: [8.741493697166442, 1.7399295874088565]) \n",
      "6800: D: -21544.544921875/11411.2412109375 G: -11684.4921875 (Real: [10.322833780050278, 4.958355147534897], Fake: [8.70526580810547, 1.9407372714604514]) \n",
      "7000: D: -20614.6015625/12670.046875 G: -12436.77734375 (Real: [10.068305969834327, 5.189497657386529], Fake: [9.419140186309814, 2.2326949169327155]) \n",
      "7200: D: -18273.0078125/13108.595703125 G: -13093.064453125 (Real: [9.374500431716442, 4.843707803460361], Fake: [8.749967453479767, 2.6730013785786273]) \n",
      "7400: D: -18713.373046875/12640.771484375 G: -12621.8505859375 (Real: [10.860050713121892, 4.7404361825311545], Fake: [9.716129388809204, 2.689214378025469]) \n",
      "7600: D: -17132.7890625/13564.4140625 G: -13140.2421875 (Real: [9.866255898326635, 5.356580336367656], Fake: [9.420614802837372, 3.2266627780362636]) \n",
      "7800: D: -14899.57421875/13040.123046875 G: -13327.5234375 (Real: [9.290693554878235, 4.90669202712098], Fake: [10.113232371807099, 3.1889611757791045]) \n",
      "8000: D: -14472.482421875/12752.0625 G: -13336.4453125 (Real: [9.922731066942214, 5.059510286926142], Fake: [10.065414979457856, 3.481118557251816]) \n",
      "8200: D: -13073.0517578125/12422.7138671875 G: -12696.958984375 (Real: [9.8880098849535, 4.971065756250991], Fake: [10.487738854885102, 3.8542737323428535]) \n",
      "8400: D: -11911.98046875/11723.26953125 G: -12421.693359375 (Real: [9.844040653053671, 5.040025027727031], Fake: [11.194649453163146, 3.803669610162116]) \n",
      "8600: D: -10866.068359375/11314.1962890625 G: -10895.875 (Real: [10.00536805331707, 5.063662522895031], Fake: [12.65267121553421, 3.6089595664874783]) \n",
      "8800: D: -9789.0810546875/10537.033203125 G: -9967.171875 (Real: [10.610857324302197, 4.72193010114454], Fake: [12.73271816253662, 4.219494453347422]) \n",
      "9000: D: -7995.90673828125/9297.04296875 G: -9214.62890625 (Real: [8.921592474449426, 5.32572473570028], Fake: [13.190137829780578, 4.0598472818256255]) \n",
      "9200: D: -7103.337890625/8076.28564453125 G: -8125.1669921875 (Real: [9.565054006576538, 5.188592765618852], Fake: [12.814975979328155, 4.507379668728664]) \n",
      "9400: D: -5979.3681640625/7252.16796875 G: -7135.16748046875 (Real: [9.555160350659863, 4.783595157152849], Fake: [13.988953039646148, 4.522204786254421]) \n",
      "9600: D: -5208.5107421875/6037.1982421875 G: -6243.908203125 (Real: [9.485620616674423, 5.437322313367336], Fake: [14.26205883026123, 4.2615758510537916]) \n",
      "9800: D: -4034.516845703125/5173.3896484375 G: -5226.578125 (Real: [8.992211234271526, 4.743858498775175], Fake: [14.243248162269593, 4.956619099322726]) \n",
      "10000: D: -3476.558349609375/4226.396484375 G: -4416.3701171875 (Real: [9.892198195941747, 4.982216356437696], Fake: [15.229026389122009, 4.763045942915809]) \n",
      "10200: D: -2970.0927734375/3411.35205078125 G: -3430.0810546875 (Real: [10.899304997324943, 5.082994017043169], Fake: [14.54747465133667, 5.689379265248192]) \n",
      "10400: D: -2203.305419921875/2611.600830078125 G: -2669.512939453125 (Real: [10.956960621476174, 4.628782957100642], Fake: [14.246865839958192, 6.105982179021079]) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10600: D: -1683.976318359375/1895.9185791015625 G: -1880.445556640625 (Real: [10.214170987606048, 5.283341349941495], Fake: [13.471195974349975, 6.618534263920208]) \n",
      "10800: D: -1082.7882080078125/1314.702392578125 G: -1286.2401123046875 (Real: [9.346726776510478, 5.132981975460081], Fake: [14.671590852737427, 6.910357497027585]) \n",
      "11000: D: -699.8746337890625/784.26953125 G: -767.029296875 (Real: [9.125654568076135, 4.829569225560545], Fake: [14.29046569943428, 7.341357480183599]) \n",
      "11200: D: -342.343017578125/397.8214111328125 G: -438.2140808105469 (Real: [10.169466665014625, 4.920132938180462], Fake: [12.937006783485412, 8.153200779891277]) \n",
      "11400: D: -138.67245483398438/96.8550796508789 G: -150.0624542236328 (Real: [10.368147209882736, 4.896909083438871], Fake: [12.129037162959575, 8.349220649160927]) \n",
      "11600: D: -167.85040283203125/33.21993637084961 G: -25.018959045410156 (Real: [9.672796524167062, 4.514977946669096], Fake: [11.772662533819675, 7.9804687000529135]) \n",
      "11800: D: -43.7341423034668/143.88552856445312 G: -53.12483215332031 (Real: [9.7233741492033, 4.2289821348235295], Fake: [9.222209049165249, 8.196787561540011]) \n",
      "12000: D: -52.019466400146484/58.918697357177734 G: -59.907997131347656 (Real: [9.994339226484298, 4.80927293864566], Fake: [9.493112070858478, 8.204974958009387]) \n",
      "12200: D: -61.136905670166016/200.90293884277344 G: -39.0618896484375 (Real: [9.578712609112262, 4.536595140730449], Fake: [7.249730932712555, 7.269294442035483]) \n",
      "12400: D: -256.3094177246094/146.37791442871094 G: -203.61451721191406 (Real: [9.503630072474479, 4.600000159019902], Fake: [7.242335000932217, 7.089392833336402]) \n",
      "12600: D: -322.6222229003906/184.55848693847656 G: -190.79815673828125 (Real: [9.536444429159165, 4.907806330195761], Fake: [7.385992321074009, 6.9905347481599724]) \n",
      "12800: D: -520.5991821289062/438.440673828125 G: -379.9372863769531 (Real: [9.592767106443643, 4.94988577849128], Fake: [4.649351771771908, 6.0752910096814094]) \n",
      "13000: D: -967.0590209960938/504.5272521972656 G: -525.4200439453125 (Real: [10.667713787257671, 4.916679802724548], Fake: [4.67580657184124, 5.745927836124226]) \n",
      "13200: D: -975.3706665039062/641.7701416015625 G: -549.0899047851562 (Real: [10.178234407901764, 5.2002000746120585], Fake: [4.732194793522358, 5.666828830533955]) \n",
      "13400: D: -1294.0198974609375/888.18212890625 G: -721.103515625 (Real: [9.847885612249375, 4.480256852364616], Fake: [3.5501278686523436, 5.238145619155493]) \n",
      "13600: D: -1376.8707275390625/909.486328125 G: -957.716064453125 (Real: [9.55035602979362, 5.39670815639364], Fake: [5.941975909471512, 6.712168180435431]) \n",
      "13800: D: -1684.5235595703125/1291.3131103515625 G: -1181.9698486328125 (Real: [9.345630244091153, 4.958393852041082], Fake: [8.387489444315433, 8.284645712028178]) \n",
      "14000: D: -2341.742431640625/1436.0955810546875 G: -1669.232177734375 (Real: [10.588199912905694, 4.357946102904963], Fake: [8.856744156181811, 8.513086946409432]) \n",
      "14200: D: -2311.237548828125/1608.6409912109375 G: -1608.9468994140625 (Real: [10.678873116075993, 4.865545817422491], Fake: [10.865878206789494, 9.445085592497085]) \n",
      "14400: D: -1738.7940673828125/1341.7548828125 G: -1485.9630126953125 (Real: [9.8716686886549, 5.401941147174676], Fake: [12.210920109450818, 9.979884156219855]) \n",
      "14600: D: -1642.6517333984375/1398.8856201171875 G: -593.9276123046875 (Real: [10.01539706647396, 4.606965655170401], Fake: [15.817749665379525, 10.08522092231661]) \n",
      "14800: D: -1146.81005859375/121.04950714111328 G: -561.9089965820312 (Real: [10.258526939153672, 4.657578401147804], Fake: [12.833704772889615, 10.451455520811136]) \n"
     ]
    }
   ],
   "source": [
    "data_mean = 10\n",
    "data_stddev = 5\n",
    "# Model params\n",
    "g_input_size = 1     # Random noise dimension coming into generator, per output vector\n",
    "g_hidden_size = 50   # Generator complexity\n",
    "g_output_size = 1    # size of generated output vector\n",
    "d_input_size = 100   # Minibatch size - cardinality of distributions\n",
    "d_hidden_size = 50   # Discriminator complexity\n",
    "d_output_size = 1    # Single dimension for 'real' vs. 'fake'\n",
    "minibatch_size = d_input_size\n",
    "d_learning_rate = 2e-4  # 2e-4\n",
    "g_learning_rate = 2e-4\n",
    "# optim_betas = (0.9, 0.999)\n",
    "num_epochs = 30000\n",
    "print_interval = 200\n",
    "# d_steps = 1  # 'k' steps in the original GAN paper. Can put the discriminator on higher training freq than generator\n",
    "d_steps = 5\n",
    "g_steps = 1\n",
    "# ### Uncomment only one of these\n",
    "#(name, preprocess, d_input_func) = (\"Raw data\", lambda data: data, lambda x: x)\n",
    "#(name, preprocess, d_input_func) = (\"Data and variances\", lambda data: decorate_with_diffs(data, 2.0), lambda x: x * 2)\n",
    "print(\"Using data [%s]\" % (name))\n",
    "# ##### DATA: Target data and generator input data\n",
    "def get_distribution_sampler(mu, sigma):\n",
    "    return lambda n: torch.Tensor(np.random.normal(mu, sigma, (1, n)))  # Gaussian\n",
    "def get_generator_input_sampler():\n",
    "    return lambda m, n: torch.rand(m, n)  # Uniform-dist data into generator, _NOT_ Gaussian\n",
    "# ##### MODELS: Generator model and discriminator model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.map1(x))\n",
    "        x = F.sigmoid(self.map2(x))\n",
    "        return self.map3(x)\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.map1(x))\n",
    "        x = F.elu(self.map2(x))\n",
    "        # return F.sigmoid(self.map3(x))\n",
    "        return self.map3(x)\n",
    "def extract(v):\n",
    "    return v.data.storage().tolist()\n",
    "def stats(d):\n",
    "    return [np.mean(d), np.std(d)]\n",
    "def decorate_with_diffs(data, exponent):\n",
    "    mean = torch.mean(data.data, 1)\n",
    "    mean_broadcast = torch.mul(torch.ones(data.size()), mean.tolist()[0][0])\n",
    "    diffs = torch.pow(data - Variable(mean_broadcast), exponent)\n",
    "    return torch.cat([data, diffs], 1)\n",
    "d_sampler = get_distribution_sampler(data_mean, data_stddev)\n",
    "gi_sampler = get_generator_input_sampler()\n",
    "G = Generator(input_size=g_input_size, hidden_size=g_hidden_size, output_size=g_output_size)\n",
    "D = Discriminator(input_size=d_input_func(d_input_size), hidden_size=d_hidden_size, output_size=d_output_size)\n",
    "# criterion = nn.BCELoss()  # Binary cross entropy: http://pytorch.org/docs/nn.html#bceloss\n",
    "# d_optimizer = optim.Adam(D.parameters(), lr=d_learning_rate, betas=optim_betas)\n",
    "# g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate, betas=optim_betas)\n",
    "#d_optimizer = optim.RMSprop(D.parameters(), lr=d_learning_rate)\n",
    "#g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate)\n",
    "lr = 5e-5\n",
    "betas = (.9, .99)\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=lr, betas=betas)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "#if use_cuda:\n",
    "    #one = one.cuda()\n",
    "    #mone = mone.cuda()\n",
    "    \n",
    "LAMBDA = .1  # Smaller lambda seems to help for toy tasks specifically\n",
    "    \n",
    "def calc_gradient_penalty(netD, real_data, fake_data):\n",
    "    alpha = torch.rand(1, minibatch_size)\n",
    "    fake_data = fake_data.view(1, 100)\n",
    "    #print(alpha.size())\n",
    "    #print(real_data.size())\n",
    "    #alpha = alpha.expand(real_data.size())\n",
    "    #print((alpha*real_data).size())\n",
    "    #print(((1 - alpha) * fake_data).size())\n",
    "\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "    #print(interpolates.size())\n",
    "\n",
    "\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "    #print(\"Interpolates size: \", interpolates.size())\n",
    "\n",
    "    disc_interpolates = netD(preprocess(interpolates))\n",
    "\n",
    "    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                              grad_outputs=torch.ones(disc_interpolates.size()),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n",
    "    return gradient_penalty\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for d_index in range(d_steps):\n",
    "        \n",
    "        #  1A: Train D on real\n",
    "        d_real_data = Variable(d_sampler(d_input_size))\n",
    "\n",
    "        \n",
    "        # 1. Train D on real+fake\n",
    "        D.zero_grad()\n",
    "        \n",
    "        d_real_decision = D(preprocess(d_real_data))\n",
    "        # d_real_error = criterion(d_real_decision, Variable(torch.ones(1)))  # ones = true\n",
    "        d_real_error = -torch.mean(d_real_decision)\n",
    "        d_real_error.backward() # compute/store gradients, but don't change params\n",
    "        \n",
    "        \n",
    "        #  1B: Train D on fake\n",
    "        d_gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n",
    "        noisev = Variable(d_gen_input, volatile=True)\n",
    "        \n",
    "        d_fake_data = G(d_gen_input).detach()  # detach to avoid training G on these labels\n",
    "        d_fake_decision = D(preprocess(d_fake_data.t()))\n",
    "        # d_fake_error = criterion(d_fake_decision, Variable(torch.zeros(1)))  # zeros = fake\n",
    "        d_fake_error = torch.mean(d_fake_decision)\n",
    "        d_fake_error.backward()\n",
    "        \n",
    "        \n",
    "        gradient_penalty = calc_gradient_penalty(D, d_real_data.data, d_fake_data.data)\n",
    "        gradient_penalty.backward()\n",
    "        D_optimizer.step()     # Only optimizes D's parameters; changes based on stored gradients from backward()\n",
    "       \n",
    "    for g_index in range(g_steps):\n",
    "        # 2. Train G on D's response (but DO NOT train D on these labels)\n",
    "        G.zero_grad()\n",
    "        gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n",
    "        g_fake_data = G(gen_input)\n",
    "        dg_fake_decision = D(preprocess(g_fake_data.t()))\n",
    "        # g_error = criterion(dg_fake_decision, Variable(torch.ones(1)))  # we want to fool, so pretend it's all genuine\n",
    "        g_error = -torch.mean(dg_fake_decision)\n",
    "        g_error.backward()\n",
    "        G_optimizer.step()  # Only optimizes G's parameters\n",
    "    if epoch % print_interval == 0:\n",
    "        print(\"%s: D: %s/%s G: %s (Real: %s, Fake: %s) \" % (epoch,\n",
    "                                                            extract(d_real_error)[0],\n",
    "                                                            extract(d_fake_error)[0],\n",
    "                                                            extract(g_error)[0],\n",
    "                                                            stats(extract(d_real_data)),\n",
    "                                                            stats(extract(d_fake_data))))\n",
    "        \n",
    "        \n",
    "plot_distribution(\"pls\", 'improved_wgan', extract(g_fake_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
